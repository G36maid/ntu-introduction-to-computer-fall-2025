# A Knowledge Distillation Framework for Real-Time Solder Ball Defect Detection

![](圖片67.jpg)

**Presented by:** 傅楸善 & 林宛萱
**Phone number:** 0910-215-621
**Email:** r14945029@ntu.edu.tw
**指導教授:** 傅楸善 教授

### Notes
各位同學大家好 我是林宛萱 接下來我要向大家報告我的研究成果…

---

## Outline
![](圖片35.jpg)
1. Introduction
2. Related Work
3. Methods
4. Results & Discussion
5. Conclusion
6. Summary

### Notes
以下是我的大鋼 接下來會分成以上這五個部分來做說明

---

## Introduction (1/3)
![](圖片37.jpg)
- Solder balls serve as critical conductive bridges between chips and circuit substrates, and their quality directly affects the electrical stability and reliability of the final product.
- Detecting solder ball defects, such as detachment, misalignment, and damage, has become indispensable in modern electronic manufacturing.

### Notes
首先是前言
在現代的電子封裝製程中，焊錫是晶片與電路板之間重要的導電橋樑。
焊錫的品質直接影響到整體產品的電性穩定度與可靠性。
所以這些焊錫的缺陷若沒有被及時發現，就可能影像產品的品質。

---

## Introduction (2/3)
**CNN: Convolutional Neural Network**

![](圖片37.jpg)
- Traditional inspection methods rely on manual visual inspection or handcrafted machine vision techniques.
- These approaches are often inefficient and prone to variations in defect morphology.
- While CNN-based methods demonstrate high accuracy, their computational demands hinder deployment on real-time edge devices.

### Notes
過去常用的檢測方式是人工目視或基於傳統電腦視覺方法來檢查。但這些方式效率低、容易受人員主觀判斷而導致誤差。
而近年來，深度學習模型在瑕疵檢測上表現優異，但是模型參數龐大、計算量高，不適合即時部署在邊緣裝置上，檢測速度也不高。
因此，我們希望設計出一個輕量化卻不犧牲準確度的模型，來檢測我們電路板上焊錫的瑕疵。

---

## Introduction (3/3)
![](圖片36.jpg)
- Achieving efficient model compression and faster inference without compromising accuracy has become a critical challenge.
- This study seeks to balance detection performance and deployment efficiency for real-time industrial inspection.

### Notes
因此本研究的目標就是去探討
如何在保持高準確率的同時，壓縮模型參數並加快推論速度。
目標是能將模型實際應用在工廠產線的邊緣裝置，以達到即時檢測的效果。

---

## Related Work (1/3)
![](圖片36.jpg)
- **Wu et al.** proposed an improved EfficientNet with a Memetic Algorithm, enhancing feature extraction and optimizing model parameters for forging defect detection.
- This study showing strong recognition performance and automation potential.

### Notes
接著是文獻回顧的部分
在瑕疵檢測領域中，已有許多研究使用 CNN 模型來達到高準確率。
W學者等人就結合了改良版 EfficientNet 與啟發式演算法，提升了鍛造元件的缺陷檢測的表現。

---

## Related Work (2/3)
![](圖片36.jpg)
- **Hinton et al.** introduced this concept to transfer knowledge via soft labels.
- **Chen et al.** proposed **SimKD**, which reuses the teacher’s classifier to directly transfer rich feature representations to the student model.
- **Zhou et al.** applied knowledge distillation techniques to industrial defect detection.
- **Knowledge distillation** reduces computational demands with little performance loss, making it well-suited for real-world industrial and embedded applications.

### Notes
而為了讓CNN模型的大小以及運算時間縮小，知識蒸餾 (Knowledge Distillation) 的概念也漸漸的被廣泛應用。
例如，H學者等人提出了透過 軟標籤轉移知識，讓模型能夠在減少參數量的前提下，表現盡可能的接近大模型。
C學者等人則進一步提出 SimKD 方法，透過重複利用 教師模型的分類器，並直接將更豐富的特徵表示傳授給學生模型。
Z學者等人更是將知識蒸餾應用於工業瑕疵檢測中，證實了知識蒸餾應用於工業設備的可行性。

---

## Knowledge Distillation - Introduction
- The **Teacher model** is first trained to learn optimal weights, which contain class information and the degree of similarity and difference between classes. These learned weights can serve as guidance for training the **Student model**.
- The objective of the **Student model's loss function** prioritizes making its predicted probability distribution as close as possible to the probability distribution generated by the **Teacher model**.

**References:**
> [1] Hinton, G., Vinyals, O., and Dean, J., "Distilling the Knowledge in a Neural Network," arXiv preprint arXiv:1503.02531, 2015.
> [2] https://medium.com/@simon3458/intro-knowledge-distillation-cea0e5d6d842

### Notes
稍微講一下知識蒸餾 (Knowledge Distillation) 的概念

**教師模型（Teacher Model）**
一般為經過充分預訓練的大型神經網路，能夠從數據中捕捉到豐富且細緻的知識。教師模型的輸出（通常為未經硬性標籤約束的邏輯或中間層特徵）包含了多層次、隱含的訊息，是知識轉移的主要來源。

**學生模型（Student Model）**
相對於教師模型，學生模型結構簡單、參數較少。學生模型透過模仿教師模型的輸出，學習其對輸入資料所做出的預測分布，以期在保持精度的同時大幅降低計算與部署成本。

Teacher 模型先完成訓練並學習到最佳權重，這些權重除了包含類別資訊，還包含了各類別彼此相異與相似的程度。可作為 Student 模型的訓練指引。

Student 模型的損失函數目標不再僅僅是最小化與原始標籤的誤差，而是優先讓其預測的機率分佈與 Teacher 模型產生的機率分佈盡可能接近

---

## Related Work (3/3)
![](圖片36.jpg)
- A classifier-guided feature distillation approach for solder ball defect detection.
- A tailored teacher-student pairing (**EfficientNet-B2** & **EfficientNet-Lite2**).
- A high-quality real-world dataset of solder ball defects.
- A complete pipeline validated from lab training to production deployment.

### Notes
基於這些文獻回顧，我們研究的主要目標有三個。
第一，我們希望設計一個知識蒸餾的訓練框架，讓輕量化模型能在邊緣裝置上保持高準確度。
第二，我們希望建立一個能真實反映產線情境的錫球缺陷資料集，並用它來驗證方法的有效性。
最後，我們希望能將這個模型實際部署在工廠環境，達成即時自動化的檢測。

---

## Methods (1/4)
![](圖片37.jpg)
**SBD-Net** is a knowledge distillation framework designed for lightweight yet accurate defect detection. The workflow consists of:

![](圖片3.jpg)

### Notes
接下來我要來介紹我們的研究方法
我們研究流程可以分成四個階段：
1. 資料集準備 (Dataset Preparation)
2. 教師模型訓練 (Teacher Model Training)
3. 學生模型蒸餾訓練 (Student Model Training with Distillation)
4. 實際部署與應用 (Deployment and Application)

---

## Methods (2/4)
![](圖片44.jpg)
The dataset contains **1,063** high-resolution images collected from a production line.
- **Normal**: 68%
- **Defective**: 32%

![](圖片36.jpg)
![](圖片38.jpg)
![](圖片40.jpg)
![](圖片5.jpg)

### Notes
首先在資料及準備的階段
我們蒐集了 1,063 張電路板的影像，並由專業人員去標記。
其中 68% 是正常樣本，32% 是缺陷樣本。
此外，我們進行了資料增強，包括旋轉、翻轉、亮度調整，以提升模型的泛化能力。
以下這些影像就是我們丟進去的影像的示意圖，其中左邊兩張是正常的樣本、右邊兩張則是有缺陷的樣本

---

## Methods (3/4)
![](圖片35.jpg)
- The **teacher network** is **EfficientNet-B2**, which incorporates multi-scale feature fusion and deeper layers to capture subtle variations of solder balls.
- The **student network** is **EfficientNet-Lite2**, a lightweight variant designed for inference efficiency.

### Notes
接著
教師模型我們選用的架構是 EfficientNet-B2，因為它能夠透過多尺度特徵融合，捕捉電路板表面的細微變化。
學生模型則是選擇 EfficientNet-Lite2。因為這個模型能在減少參數與運算成本的同時，維持良好的特徵表示能力，非常適合部署在邊緣裝置。

---

## Methods (4/4)
![](圖片36.jpg)
- Unlike traditional KD, **SimKD** reuses the teacher classifier weights.
- This classifier-guided distillation allows the student to inherit discriminative features directly, preserving structural knowledge effectively.

The student model was deployed on an edge device integrated into a production line.
![](圖片1.jpg)

### Notes
接著是我們的知識蒸餾方法，我們參考了SimKD的知識蒸餾策略，重複使用教師模型的分類器權重，讓學生模型不僅能學到輸出結果，也能學到教師模型內部的結構化知識。
上面這個就是我們訓練時的loss函數

最後，我們將學生模型部署在 Jetson Xavier NX 邊緣裝置。
搭配高速相機與控制單元，讓系統能即時辨識有缺陷的焊錫，並觸發警示，做到即時檢測的功能。

---

## Results & Discussion (1/4)
![](圖片36.jpg)
- **Dataset split**: 70% training, 15% validation, 15% testing
- **Hardware**: NVIDIA RTX 2080Ti GPU
- **Framework**: PyTorch
- Standardized preprocessing pipeline

### Notes
接著是實驗結果與討論的部分
我們的實驗環境如投影片上所示
資料集分割為訓練 70%、驗證 15%、測試 15%。
並且使用 NVIDIA RTX 2080Ti GPU 與 PyTorch 框架。
而且所有資料都是經過標準化與資料增強的影像。

---

## Results & Discussion (2/4)
![](圖片38.jpg)
**34% reduction in parameters, 1.4× faster inference.**

| Model | Parameters (M) ↓ | Inference Time (ms) ↓ |
| :--- | :--- | :--- |
| **Teacher** (EfficientNet-B2) | 9.2 | 52 |
| **Student** (EfficientNet-Lite2) | 6.1 | 37 |

### Notes
在模型效率方面，教師模型 EfficientNet-B2 的參數量為九百二十萬，單張影像推論需要五十二毫秒。
而學生模型 EfficientNet-Lite2 的參數量只有六百一十萬，推論時間則縮短為三十七毫秒。
整體來說，參數減少了三十四％，推論速度則提升了約一點四倍。

---

## Results & Discussion (3/4)
- **Acc**: Accuracy
- **Prec**: Precision
- **Rec**: Recall
- **F1**: F1-score

![](圖片36.jpg)

| Model | Acc (%) ↑ | Prec (%) ↑ | Rec (%) ↑ | F1 (%) ↑ |
| :--- | :--- | :--- | :--- | :--- |
| **EfficientNet-B2** | 99.6 | 100.0 | 95.0 | 97.4 |
| **EfficientNet-Lite2 (original)** | 98.0 | 86.3 | 91.3 | 88.6 |
| **EfficientNet-Lite2 (Traditional KD)** | 99.0 | 96.9 | 91.3 | 94.0 |
| **EfficientNet-Lite2 (Our Method)** | 99.6 | 100.0 | 95.0 | 97.4 |

### Notes
在分類結果的表現方面，教師模型，也就是EfficientNet-B2，在測試集上的準確率為九十九點六％。
如果EfficientNet-Lite2沒有經過蒸餾訓練，準確率只有九十八％。
使用傳統知識蒸餾後，準確率提升到九十九％。
而我們提出的 SBD-Net 則讓學生模型達到與教師模型相同的九十九點六％，顯示出在壓縮模型的同時，我們仍能保持教師模型的準確度。

---

## Results & Discussion (4/4)
![](圖片38.jpg)
- **Our Method** preserved high accuracy despite model compression.
- **Original student model** suffered performance loss; KD significantly improved results.
- **Recall** remained limited due to dataset imbalance and subtle defect textures.

### Notes
經過剛剛實驗結果的觀察，我們可以看到使用我們的方法進行模型壓縮之後仍然可以維持很高的準確率。
但即使如此，由於缺陷樣本在資料集中仍然屬於少數，所以學生模型在某些罕見類別上的偵測還是會受到限制。
這將會是我們這個研究未來的改進方向和目標

---

## Conclusion
![](圖片38.jpg)
**SBD-Net** effectively reuses the teacher classifier to guide student training, achieving:
- **34%** reduction in parameters
- Faster inference
- **99.6%** accuracy in test set
- Successful real-world deployment on edge devices

The framework demonstrates practical feasibility for real-time solder ball defect detection in industrial environments.

### Notes
總結來說，我們提出的 SBD-Net 框架，成功在參數減少三十四％、推論速度提升的同時，維持了九十九點六％的高準確率，
並且能夠在真實的工廠產線上完成部署，達成即時檢測的目標。
這表現出了我們的研究在工業應用中的實際應用價值，可以為及時自動化檢測提供了一個可行的解決方案。

---

## Summary
![](圖片38.jpg)
**Method**
- Uses EfficientNet-B2 (teacher) and EfficientNet-Lite2 (student)
- Knowledge distillation via SimKD
- Deployed on real production edge devices

**Steps**
1. Dataset Preparation
2. Teacher Model Training
3. Student Model Training with Distillation
4. Deployment and Application

**Results**
- 34% fewer parameters
- 1.4× faster inference
- Maintained the same accuracy as the teacher model

### Notes
接下來的這一頁
我將針對本研究的「方法」、「步驟」與「結果」做一個重點整理。
這一頁的內容會列入考試範圍，請大家要特別注意一下！
首先在方法部分，
我們使用EfficientNet-B2作為教師模型，EfficientNet-Lite2作為學生模型，並且參考了SimKD的方法來知識蒸餾。最後將學生模型實際部署在生產線上的邊緣裝置。
步驟方面，
我們先進行資料準備，包括蒐集並標註焊球影像。接著訓練教師模型，再透過知識蒸餾訓練學生模型，最後將學生模型應用到實際的邊緣裝置上進行即時缺陷檢測。
在結果方面，
SBD-Net讓模型的參數量減少了34%，推論速度提升1.4倍，而且學生模型在測試集上，仍然維持與教師模型一樣的準確率。這個方法已經在真實的產線環境中應用，證明它的實用性與可行性。

---

## References
1. Y. He, K. Song, Q. Meng, and Y. Yan, “An end-to-end steel surface defect detection approach via fusing multiple hierarchical features,” IEEE Trans. Instrum. Meas., vol. 69, no. 4, pp. 1493–1504, Apr. 2020.
2. C. Li, B. Jiang, Z. Liu, Y. Dong, S. Tang, D. Weng, “Fabric defect detection based on deep-handcrafted feature and weighted low-rank matrix representation,” Journal of Engineered Fibers and Fabrics, vol. 6, pp. 155892502110084-155892502110084, 2021.
3. D. Zhang, X. Hao, D. Wang, C. Qin, B. Zhao, L. Liang, and W. Liu, “An efficient lightweight convolutional neural network for industrial surface defect detection,” Artificial Intelligence Review, vol. 56, pp. 1-27, 2023.
4. H. Zhang, D. Pa, J. Liu, and Z. Jiang, “A novel MAS-GAN-based data synthesis method for object surface defect detection,” Neurocomputing, vol. 499, pp. 106-144, 2022.
5. K. Volkan and I. Akgül, “Detection of defects in printed circuit boards with machine learning and deep learning algorithms,” European Journal of Science and Technology, no. 41, pp. 183–186, Nov. 2022.
6. R. Ameri, C. C. Hsu, and S. S. Band, “A systematic review of deep learning approaches for surface defect detection in industrial applications,” Engineering Applications of Artificial Intelligence, vol. 130, pp. 107717, 2024.
7. T. Yu, W. Chen, G. Junfeng and H. Poxi, “Intelligent Detection Method of Forgings Defects Detection Based on Improved EfficientNet and Memetic Algorithm,” in IEEE Access, vol. 10, pp. 79553-79563, 2022.
8. P. V. Dantas, W. Sabino da Silva Jr, L. C. Cordeiro, and C. B. Carvalho, “A comprehensive review of model compression techniques in machine learning,” Appl Intell, vol. 54, pp. 11804–11844, 2024.
9. Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A Survey of Model Compression and Acceleration for Deep Neural Networks,” IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 126-136, Jan. 2018.
10. X. Dai, H. Yin, and N. K. Jha, “NeST: A neural network synthesis tool based on a grow-and-prune paradigm,” IEEE Trans. Comput., vol. 68, no. 10, pp. 1487–1497, Oct. 2019.
11. Z. Wang, H. Chen, S. Yang, X. Luo, D. Li, and J. Wang, “A lightweight intrusion detection method for IoT based on deep learning and dynamic quantization,” PeerJ Computer Science, vol. 9, pp. 1569, 2023.
12. G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 2015, arXiv:1503.02531.
13. Y. Tian, S. Pei, X. Zhang, C. Zhang, and N. Chawla, “Knowledge Distillation on Graphs: A Survey,” ACM Computing Surveys, Jan. 2025.
14. J. Gou, L. Sun, B. Yu, S. Wan, and D. Tao, “Hierarchical Multi-Attention Transfer for Knowledge Distillation,” ACM Transactions on Multimedia Computing Communications and Applications, vol. 20, no. 2, pp. 1–20, Oct. 2022.
15. Q. Zhou, H. Wang, Y. Tang and Y. Wang, “Defect Detection Method Based on Knowledge Distillation,” in IEEE Access, vol. 11, pp. 35866-35873, 2023.
16. D. Chen, J.-P. Mei, H. Zhang, C. Wang, Y. Feng, and C. Chen, “Knowledge Distillation with the Reused Teacher Classifier,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2022.

---

## Thank You
![](圖片36.jpg)

**Presented by:** 傅楸善 & 林宛萱
**Phone number:** 0910-215-621
**Email:** r14945029@ntu.edu.tw
**指導教授:** 傅楸善 教授

### Notes
以上是我的報告，謝謝各位同學的聆聽~~
